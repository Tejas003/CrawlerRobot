# CrawlerRobot
Python code to extract User agent and its disallowed and allowed directories from any site
It takes user entered Url and checks if robots.txt file is present on the site or not
If its not present, it quits printing file is not present
Else it creates a Csv file where it creates three columns as Agent, Allow, Disallow

#Files used 
Crawler_Python_Socket.py -> Main File
Robot.csv -> CSV table created by the code

#Libraries Used
urllib.request
re




